method = "fluxbot",
volume = 9258.099,
area = 646.328) %>%
remove_rownames() %>%
mutate(autochamber = "12")
fluxes_HFarray <- rbind(fluxes_HF1, fluxes_HF2, fluxes_HF3, fluxes_HF4, fluxes_HF5, fluxes_HF6,
fluxes_HF7, fluxes_HF8, fluxes_HF9, fluxes_HF10, fluxes_HF11, fluxes_HF12)
View(fluxes_HFarray)
write.csv(fluxes_HFarray, "HFarray_fluxes_calculatedusingfluxbotcode.csv", col.names = TRUE)
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(readr)
library(readxl)
library(ggpubr)
library(here)
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(readr)
library(readxl)
library(ggpubr)
library(here)
library(tidyverse)
library(readxl)
library(here)
library(purrr)
library(tidyverse)
library(readxl)
library(here)
library(purrr)
# function to import a single xlsx file with each tab as a separate df in a list of dfs with the name of the tab as the df name
read_excel_allsheets <- function(filename, tibble = FALSE) {
# I prefer straight data.frames
# but if you like tidyverse tibbles (the default with read_excel)
# then just pass tibble = TRUE
sheets <- readxl::excel_sheets(filename)
x <- lapply(sheets, function(X) readxl::read_excel(filename, sheet = X))
if(!tibble) x <- lapply(x, as.data.frame)
names(x) <- sheets
x
}
# source:
# https://stackoverflow.com/questions/12945687/read-all-worksheets-in-an-excel-workbook-into-an-r-list-with-data-frames
co2 <- read_excel_allsheets(here("fluxbot data 2023", "fluxbot_co2.xlsx"),
tibble = FALSE)
rh <- read_excel_allsheets(here("fluxbot data 2023", "fluxbot_humidity.xlsx"),
tibble = FALSE)
temp <- read_excel_allsheets(here("fluxbot data 2023", "fluxbot_temperature.xlsx"),
tibble = FALSE)
press <- read_excel_allsheets(here("fluxbot data 2023", "fluxbot_pressure.xlsx"),
tibble = FALSE)
# file named "process_googledata_list.R" in folder "scripts_batch processing fluxbot data" in this directory
source(here("scripts_batch processing fluxbot data", "process_googledata_list.R"))
co2_rawdata <- process_data(co2, col1 = "device timestamps", col2 = "co2")
rh_rawdata <- process_data(rh, col1 = "device timestamps", col2 = "humidity")
temp_rawdata <- process_data(temp, col1 = "device timestamps", col2 = "temprerature") # edit to note: there's a typo in the fluxbot software that produces a column named temprerature in the output on Google forms; need to fix
press_rawdata <- process_data(press, col1 = "device timestamps", col2 = "pressure")
View(co2_rawdata)
View(press_rawdata)
# file named "merge_metadata_list.R" in folder "scripts_batch processing fluxbot data" in this directory
source(here("scripts_batch processing fluxbot data", "merge_metadata_list.R"))
merged_result <- merge_lists_by_column(co2_rawdata, rh_rawdata, temp_rawdata, press_rawdata, "UNIX")
View(merged_result)
# Function to change column names and convert specified columns to numeric
change_colnames_and_convert <- function(lst) {
lapply(lst, function(df) {
# Change column names
colnames(df) <- c("Time", "co2", "humidity", "tempC", "pressure")
# Convert specific columns to numeric (assuming they are character)
df$Time <- as.numeric(df$Time)
df$Time <- as.POSIXct(df$Time, format = "%Y-%m-%d %H:%M:%S", tz = "America/New_York")
df$co2 <- as.numeric(df$co2)
df$humidity <- as.numeric(df$humidity)
df$tempC <- as.numeric(df$tempC)
df$pressure <- as.numeric(df$pressure)
return(df)
})
}
# Apply the function to the list of lists: result will be a list of dataframes with all indicated columns of class "numeric", and updated colnames
allfluxbotdata_list <- change_colnames_and_convert(merged_result)
View(allfluxbotdata_list)
# extract all datetimes, then find min and max of all:
all_datetimes <- unlist(lapply(allfluxbotdata_list, function(df) df$Time))
# find earliest (min) and latest (max) datetime after deployment in HF:
# first define start date of deployment:
startdate <- as.POSIXct("2023-10-01 00:00:00 EDT")
# filter data for that after startdate defined above:
all_datetimesfiltered <- all_datetimes[all_datetimes > startdate]
as.POSIXct(min(all_datetimesfiltered))
# [1] "2023-10-01 00:00:02 EDT"
as.POSIXct(max(all_datetimesfiltered))
# generate a viable start-end df: this will have start/end brackets for the wanted observation intervals for every hour between the earliest and latest datetimes identified above (this can also be determined manually)
start_datetime <- as.POSIXct("2023-10-01 00:00:02")
end_datetime <- as.POSIXct("2023-11-04 01:00:09")
interval_length <- as.difftime(1, units = "hours")  # 1 hour interval
# function titled "extract_startend_intervals_df.R" in folder "scripts_batch processing fluxbot data" in this directory
start_end <- generate_interval_df(start_datetime, end_datetime,
interval_length,
start_minute = 56, start_second = 00,
end_minute = 59, end_second = 59)
# function titled "extract_startend_intervals_df.R" in folder "scripts_batch processing fluxbot data" in this directory
source(here("scripts_batch processing fluxbot data", "extract_startend_intervals_df.R"))
start_end <- generate_interval_df(start_datetime, end_datetime,
interval_length,
start_minute = 56, start_second = 00,
end_minute = 59, end_second = 59)
# Produce pressure data from HF met station:
# load HF met data from 2016 - 2017 and convert to useful units
met_data <- read.csv("https://harvardforest.fas.harvard.edu/data/p00/hf001/hf001-10-15min-m.csv")
met_data$ymd <- ymd(str_split_fixed(met_data$datetime, "T", 2)[,1])
met_data$time <- hm(str_split_fixed(met_data$datetime, "T", 2)[,2])
met_data <- met_data[which(met_data$ymd>=ymd("2023-06-01") & met_data$ymd<ymd("2023-11-30")),]
#unit conversions: not necessary as mbar = hPa (just rename)
met_data <- met_data %>%
select(datetime, bar, airt, prec, s10t) %>%
rename(pressure_hPa_HF = bar,
tempC_HF = airt,
precip_HF = prec,
soiltemp_10cm_HF = s10t)
met_data <- met_data %>%
mutate(Time = as.POSIXct(datetime,
format = "%Y-%m-%dT%H:%M",
tz = "America/New_York"))
# Define a function to apply the process to a single data frame
join_met_data <- function(df) {
df <- df %>%
left_join(met_data,
join_by(closest(Time>=Time))) %>%
# by = c("closest" = "Time")) %>%
select(Time.x, co2, humidity, tempC, pressure,
pressure_hPa_HF, tempC_HF, precip_HF, soiltemp_10cm_HF) %>%
rename(Time = Time.x,
pressure_onboard = pressure,
pressure = pressure_hPa_HF)
return(df)
}
rawdata_bots <- allfluxbotdata_list
# Apply the function to each data frame in the list
fluxbots_metdata_list <- lapply(rawdata_bots,
join_met_data)
View(fluxbots_metdata_list)
# Write each data frame in the resulting list to the global environment with its original name
for (i in seq_along(fluxbots_metdata_list)) {
assign(names(rawdata_bots)[i], fluxbots_metdata_list[[i]])
}
# fluxbots in the healthy plot:
fluxes_bot100 <- calculate_regressions_custom_intervals(rawdata_fluxbot_100, start_end, "fluxbot", 768, 81)
# Append the results to the dataframe
results_df <- rbind(results_df, data.frame(
start_timestamp = start_point,
end_timestamp = end_point,
hour_of_obs = round(end_point, "hour"),
method = method,
# adding starting concentration to output, June 18 2025:
starting_concen = interval_data[1],
linear_beta = linear_beta, # kg/s
quadratic_beta = quadratic_beta, # kg/s
length_interval = length_interval, # total interval length in seconds
delta_kg_L = linear_beta/length_interval, # units in kg
delta_kg_Q  = quadratic_beta/length_interval, # units in kg
fluxL_kgm2sec = fluxL, # flux estimate in kg/m2/sec
fluxQ_kgm2sec = fluxQ, # flux estimate in kg/m2/sec
fluxL_umolm2sec = fluxL_umol, # flux estimate in umol/m2/sec
fluxQ_umolm2sec = fluxQ_umol  # flux estimate in umol/m2/sec
))
calculate_regressions_custom_intervals <- function(time_series_df, custom_intervals_df,
method, volume, area) {
# Remove rows with NAs
time_series_df <- time_series_df[complete.cases(time_series_df), ]
# Extract timestamp and data columns from the time series dataframe
timestamp_col <- as.POSIXct(time_series_df$Time, format = "%Y-%m-%d %H:%M:%S", tz = "America/New_York")
data_col <- time_series_df$co2
# Initialize an empty dataframe to store the results
results_df <- data.frame(
start_timestamp = character(),
end_timestamp = character(),
linear_beta = numeric(),
quadratic_beta = numeric()
)
# Quality control thresholds: if an interval 'fails' any of these it is skipped and excluded from the final output
diff_threshold <- 5 # difference in co2 concentration delta from start-end needs to be greater than 5ppm
max_threshold <- 3000 # eliminate any intervals that contain co2 concentrations greater than 3k ppm
length_threshold <- 15 # the length of an interval needs to be at least 15 observations (40 obs per 4mins interval for fluxbot)
# constants for conversion of concentration to mass:
R_m = 8.314472  # Ideal gas constant for mass [(m3*Pa)/(K*mol)]
R_specific <- 287.058 # specific gas constant for dry air (J/kg*K)
mol_mass <- 28.9628 # molar mass dry air, g/mol
volume <- volume # cm3; volume of fluxbot chamber, typically 768cm3 without additional tubing/attachments
area <- area # cm2; area of soil that fluxbot collar encompasses, typically 81cm2
g_per_mol = 44.009 # molar mass of CO2, g/mol
# Loop through each row in the custom intervals dataframe to ID the start and end point of each interval
for (i in seq_len(nrow(custom_intervals_df))) {
# Extract start and end points for the current row
start_point <- custom_intervals_df$Start[i]
end_point <- custom_intervals_df$End[i]
# Find the closest timestamp to start_point and end_point in the time series data itself:
start_index <- which.min(abs(timestamp_col - start_point))
end_index <- which.min(abs(timestamp_col - end_point))
# Check if the closest start and end points are within 30 minutes
if (abs(start_point - timestamp_col[start_index]) > 1800 || abs(end_point - timestamp_col[end_index]) > 1800) {
cat("Skipping interval due to no close timepoints:", j, "\n")
next  # Skip to the next iteration
}
# Extract the CO2 concentration data for each interval iteratively using the above timestamps:
interval_data <- time_series_df$co2[start_index:end_index]
interval_timestamp <- time_series_df$Time[start_index:end_index]
# identify the interval pressure to run QAQC too:
interval_pressure <- time_series_df$pressure[start_index:end_index]
# determine total length of interval; reports in mins, so convert to sec:
length_interval <- (as.numeric(time_series_df$Time[end_index] - time_series_df$Time[start_index]))*60
# Quality control checks with printouts of reasons for excluding a given interval:
if (any(is.na(interval_data))) {
cat("Skipping interval due to NAs in the data:", i, "\n")
next  # Skip to the next iteration
}
if (abs(interval_data[1] - interval_data[length(interval_data)]) < diff_threshold) {
cat("Skipping interval due to extremely small delta:", i, "\n")
next  # Skip to the next iteration
}
if (interval_data[length(interval_data)] - interval_data[1] < 0){
cat("Skipping interval due to negative delta:", i, "\n")
next
}
# if(max(interval_data) > max_threshold) {
# cat("Skipping interval: concentration too high:", i, "\n")
# next # Skip to the next iteration
# }
if(length(interval_data) < length_threshold) {
cat("Skipping interval due to insufficient length:", i, "\n")
next # Skip to the next iteration
}
# temporary fix for intervals with weird pressure data, March 4th 2024,
# while I figure out how to sub in good pressure data when needed
# Implement quality control check for pressure data
if (any(interval_pressure > 1050) || any(interval_pressure < 965)) {
cat("Skipping interval due to error in pressure data:", i, "\n")
next  # Skip to the next iteration
}
# Remove rows with specific value for co2 indicating a data transmission error, within the interval data
interval_data <- interval_data[interval_data != 65535]
interval_data <- interval_data[interval_data != 65533]
# calculate rho_a, air density in kg/m3, including humidity:
# first step, calculate saturation vapor pressure in hPa
e_star <- 10*(0.61978 * exp((17.27 * time_series_df$tempC)[start_index:end_index]/
(time_series_df$tempC[start_index:end_index] +
273.3))) # saturation vapor pressure, hPa
# next: saturation density (kg water / kg air) given saturation vapor pressure (e_star) and observed air pressure
# 0.62198 = ratio of molecular masses of water and dry air (18.01528 / 28.9645)
x_s <- 0.62198 * e_star / (time_series_df$pressure[start_index:end_index] - e_star)
# next: calculate humidity ratio using saturation density and observed relative humidity:
x <- x_s * time_series_df$humidity[start_index:end_index] / 100 # divide by 100 to convert to a fraction
# finally, calculate the observed density of humidity-corrected air in the chamber:
# first: density of air in kg/m3 using specific gas constant for dry air
rho_d <- (time_series_df$pressure[start_index:end_index] * 100) /
(R_specific * (time_series_df$tempC[start_index:end_index] + 273.15)) # here converting T to Kelvin from Celcius
# air density in kg/m3 with humidity correction as calculated in "x" above:
# 1.609 = gas constant ratio between water vapor and air
rho_a <- rho_d * (1+x) / (1 + 1.609 * x)
# convert (general) co2 in ppm to mol/m3
mol_kg = 1/(mol_mass/1000) # mass of air in mol/kg
# calculate mol of moist air per m3 in the chamber given previously-calculated moist air densities
mol_m3 = mol_kg * rho_a # mass of moist air in chamber in mol/m3
# convert (our observed) co2 in ppm to molar density using chamber volume:
mol_gas_m3 <- mol_m3 * (interval_data/1000000) # returns mol/m3 (conversion from cm3 to m3 = 1,000,000)
# convert molar concentration of CO2 into concentration in kg/m3
kg_gas_m3 <- mol_gas_m3 * (g_per_mol/1000) # (conversion of g to kg = 1000)
# convert to just mass in kg (volume reported in cm3, converting to m3 here, inverse conversion = 0.000001):
kg_gas <- kg_gas_m3 * (volume * 0.000001) # returns kg of co2 in the chamber at each time point in interval
# Linear regression
linear_model <- lm(kg_gas ~ as.numeric(difftime(interval_timestamp, min(interval_timestamp), units = "secs")))
linear_results <- summary(linear_model)
linear_beta <- coef(linear_results)[2]  # Extract the beta coefficient; kg/s
delta_kg_L = linear_beta*(length_interval) # units in kg
delta_g_L = delta_kg_L*1000 # units in g
delta_mol_L = delta_g_L / g_per_mol # units in mols
fluxL = delta_kg_L / length_interval / (area*0.0001) # flux estimate in kg/m2/sec
fluxL_umol = (delta_mol_L*1000000) / length_interval / (area * 0.0001) # flux estimate in umol/m2/sec
# Quadratic regression
quadratic_model <- lm(kg_gas ~
as.numeric(difftime(interval_timestamp, min(interval_timestamp),
units = "secs")) +
I(as.numeric(difftime(interval_timestamp,
min(interval_timestamp),
units = "secs"))^2))
quadratic_results <- summary(quadratic_model)
quadratic_beta <- coef(quadratic_results)[2]  # Extract the beta coefficient; kg/s
delta_kg_Q  = quadratic_beta*(length_interval) # units in kg
delta_g_Q = delta_kg_Q*1000 # units in g
delta_mol_Q = delta_g_Q / g_per_mol # units in mols
fluxQ = delta_kg_Q / length_interval / (area*0.0001) # flux estimate in kg/m2/sec
fluxQ_umol = (delta_mol_Q*1000000) / length_interval / (area * 0.0001) # flux estimate in umol/m2/sec
# Append the results to the dataframe
results_df <- rbind(results_df, data.frame(
start_timestamp = start_point,
end_timestamp = end_point,
hour_of_obs = round(end_point, "hour"),
method = method,
# adding starting concentration to output, June 18 2025:
starting_concen = interval_data[1],
linear_beta = linear_beta, # kg/s
quadratic_beta = quadratic_beta, # kg/s
length_interval = length_interval, # total interval length in seconds
delta_kg_L = linear_beta/length_interval, # units in kg
delta_kg_Q  = quadratic_beta/length_interval, # units in kg
fluxL_kgm2sec = fluxL, # flux estimate in kg/m2/sec
fluxQ_kgm2sec = fluxQ, # flux estimate in kg/m2/sec
fluxL_umolm2sec = fluxL_umol, # flux estimate in umol/m2/sec
fluxQ_umolm2sec = fluxQ_umol  # flux estimate in umol/m2/sec
))
}
# Return the results
return(results_df)
}
# function titled "estimatefluxes.R" in folder "scripts_batch processing fluxbot data" in this directory:
source(here("scripts_batch processing fluxbot data", "estimatefluxes.R"))
# fluxbots in the healthy plot:
fluxes_bot100 <- calculate_regressions_custom_intervals(rawdata_fluxbot_100, start_end, "fluxbot", 768, 81)
View(fluxes_bot100)
calculate_regressions_custom_intervals <- function(time_series_df, custom_intervals_df,
method, volume, area) {
# Remove rows with NAs
time_series_df <- time_series_df[complete.cases(time_series_df), ]
# Extract timestamp and data columns from the time series dataframe
timestamp_col <- as.POSIXct(time_series_df$Time, format = "%Y-%m-%d %H:%M:%S", tz = "America/New_York")
data_col <- time_series_df$co2
# Initialize an empty dataframe to store the results
results_df <- data.frame(
start_timestamp = character(),
end_timestamp = character(),
linear_beta = numeric(),
quadratic_beta = numeric()
)
# Quality control thresholds: if an interval 'fails' any of these it is skipped and excluded from the final output
diff_threshold <- 5 # difference in co2 concentration delta from start-end needs to be greater than 5ppm
max_threshold <- 3000 # eliminate any intervals that contain co2 concentrations greater than 3k ppm
length_threshold <- 15 # the length of an interval needs to be at least 15 observations (40 obs per 4mins interval for fluxbot)
# constants for conversion of concentration to mass:
R_m = 8.314472  # Ideal gas constant for mass [(m3*Pa)/(K*mol)]
R_specific <- 287.058 # specific gas constant for dry air (J/kg*K)
mol_mass <- 28.9628 # molar mass dry air, g/mol
volume <- volume # cm3; volume of fluxbot chamber, typically 768cm3 without additional tubing/attachments
area <- area # cm2; area of soil that fluxbot collar encompasses, typically 81cm2
g_per_mol = 44.009 # molar mass of CO2, g/mol
# Loop through each row in the custom intervals dataframe to ID the start and end point of each interval
for (i in seq_len(nrow(custom_intervals_df))) {
# Extract start and end points for the current row
start_point <- custom_intervals_df$Start[i]
end_point <- custom_intervals_df$End[i]
# Find the closest timestamp to start_point and end_point in the time series data itself:
start_index <- which.min(abs(timestamp_col - start_point))
end_index <- which.min(abs(timestamp_col - end_point))
# Check if the closest start and end points are within 30 minutes
if (abs(start_point - timestamp_col[start_index]) > 1800 || abs(end_point - timestamp_col[end_index]) > 1800) {
cat("Skipping interval due to no close timepoints:", j, "\n")
next  # Skip to the next iteration
}
# Extract the CO2 concentration data for each interval iteratively using the above timestamps:
interval_data <- time_series_df$co2[start_index:end_index]
interval_timestamp <- time_series_df$Time[start_index:end_index]
# identify the interval pressure to run QAQC too:
interval_pressure <- time_series_df$pressure[start_index:end_index]
# determine total length of interval; reports in mins, so convert to sec:
length_interval <- (as.numeric(time_series_df$Time[end_index] - time_series_df$Time[start_index]))*60
# Quality control checks with printouts of reasons for excluding a given interval:
if (any(is.na(interval_data))) {
cat("Skipping interval due to NAs in the data:", i, "\n")
next  # Skip to the next iteration
}
if (abs(interval_data[1] - interval_data[length(interval_data)]) < diff_threshold) {
cat("Skipping interval due to extremely small delta:", i, "\n")
next  # Skip to the next iteration
}
if (interval_data[length(interval_data)] - interval_data[1] < 0){
cat("Skipping interval due to negative delta:", i, "\n")
next
}
# if(max(interval_data) > max_threshold) {
# cat("Skipping interval: concentration too high:", i, "\n")
# next # Skip to the next iteration
# }
if(length(interval_data) < length_threshold) {
cat("Skipping interval due to insufficient length:", i, "\n")
next # Skip to the next iteration
}
# temporary fix for intervals with weird pressure data, March 4th 2024,
# while I figure out how to sub in good pressure data when needed
# Implement quality control check for pressure data
if (any(interval_pressure > 1050) || any(interval_pressure < 965)) {
cat("Skipping interval due to error in pressure data:", i, "\n")
next  # Skip to the next iteration
}
# Remove rows with specific value for co2 indicating a data transmission error, within the interval data
interval_data <- interval_data[interval_data != 65535]
interval_data <- interval_data[interval_data != 65533]
# calculate rho_a, air density in kg/m3, including humidity:
# first step, calculate saturation vapor pressure in hPa
e_star <- 10*(0.61978 * exp((17.27 * time_series_df$tempC)[start_index:end_index]/
(time_series_df$tempC[start_index:end_index] +
273.3))) # saturation vapor pressure, hPa
# next: saturation density (kg water / kg air) given saturation vapor pressure (e_star) and observed air pressure
# 0.62198 = ratio of molecular masses of water and dry air (18.01528 / 28.9645)
x_s <- 0.62198 * e_star / (time_series_df$pressure[start_index:end_index] - e_star)
# next: calculate humidity ratio using saturation density and observed relative humidity:
x <- x_s * time_series_df$humidity[start_index:end_index] / 100 # divide by 100 to convert to a fraction
# finally, calculate the observed density of humidity-corrected air in the chamber:
# first: density of air in kg/m3 using specific gas constant for dry air
rho_d <- (time_series_df$pressure[start_index:end_index] * 100) /
(R_specific * (time_series_df$tempC[start_index:end_index] + 273.15)) # here converting T to Kelvin from Celcius
# air density in kg/m3 with humidity correction as calculated in "x" above:
# 1.609 = gas constant ratio between water vapor and air
rho_a <- rho_d * (1+x) / (1 + 1.609 * x)
# convert (general) co2 in ppm to mol/m3
mol_kg = 1/(mol_mass/1000) # mass of air in mol/kg
# calculate mol of moist air per m3 in the chamber given previously-calculated moist air densities
mol_m3 = mol_kg * rho_a # mass of moist air in chamber in mol/m3
# convert (our observed) co2 in ppm to molar density using chamber volume:
mol_gas_m3 <- mol_m3 * (interval_data/1000000) # returns mol/m3 (conversion from cm3 to m3 = 1,000,000)
# convert molar concentration of CO2 into concentration in kg/m3
kg_gas_m3 <- mol_gas_m3 * (g_per_mol/1000) # (conversion of g to kg = 1000)
# convert to just mass in kg (volume reported in cm3, converting to m3 here, inverse conversion = 0.000001):
kg_gas <- kg_gas_m3 * (volume * 0.000001) # returns kg of co2 in the chamber at each time point in interval
# Linear regression
linear_model <- lm(kg_gas ~ as.numeric(difftime(interval_timestamp, min(interval_timestamp), units = "secs")))
linear_results <- summary(linear_model)
linear_beta <- coef(linear_results)[2]  # Extract the beta coefficient; kg/s
delta_kg_L = linear_beta*(length_interval) # units in kg
delta_g_L = delta_kg_L*1000 # units in g
delta_mol_L = delta_g_L / g_per_mol # units in mols
fluxL = delta_kg_L / length_interval / (area*0.0001) # flux estimate in kg/m2/sec
fluxL_umol = (delta_mol_L*1000000) / length_interval / (area * 0.0001) # flux estimate in umol/m2/sec
# Quadratic regression
quadratic_model <- lm(kg_gas ~
as.numeric(difftime(interval_timestamp, min(interval_timestamp),
units = "secs")) +
I(as.numeric(difftime(interval_timestamp,
min(interval_timestamp),
units = "secs"))^2))
quadratic_results <- summary(quadratic_model)
quadratic_beta <- coef(quadratic_results)[2]  # Extract the beta coefficient; kg/s
delta_kg_Q  = quadratic_beta*(length_interval) # units in kg
delta_g_Q = delta_kg_Q*1000 # units in g
delta_mol_Q = delta_g_Q / g_per_mol # units in mols
fluxQ = delta_kg_Q / length_interval / (area*0.0001) # flux estimate in kg/m2/sec
fluxQ_umol = (delta_mol_Q*1000000) / length_interval / (area * 0.0001) # flux estimate in umol/m2/sec
# Append the results to the dataframe
results_df <- rbind(results_df, data.frame(
start_timestamp = start_point,
end_timestamp = end_point,
hour_of_obs = round(end_point, "hour"),
method = method,
# adding starting and ending concentration to output, June 18 2025:
starting_concen = interval_data[1],
ending_concen = interval_data[length(interval_data)],
linear_beta = linear_beta, # kg/s
quadratic_beta = quadratic_beta, # kg/s
length_interval = length_interval, # total interval length in seconds
delta_kg_L = linear_beta/length_interval, # units in kg
delta_kg_Q  = quadratic_beta/length_interval, # units in kg
fluxL_kgm2sec = fluxL, # flux estimate in kg/m2/sec
fluxQ_kgm2sec = fluxQ, # flux estimate in kg/m2/sec
fluxL_umolm2sec = fluxL_umol, # flux estimate in umol/m2/sec
fluxQ_umolm2sec = fluxQ_umol  # flux estimate in umol/m2/sec
))
}
# Return the results
return(results_df)
}
# function titled "estimatefluxes.R" in folder "scripts_batch processing fluxbot data" in this directory:
source(here("scripts_batch processing fluxbot data", "estimatefluxes.R"))
# fluxbots in the healthy plot:
fluxes_bot100 <- calculate_regressions_custom_intervals(rawdata_fluxbot_100, start_end, "fluxbot", 768, 81)
fluxes_bot13 <- calculate_regressions_custom_intervals(rawdata_fluxbot_13, start_end, "fluxbot", 768, 81)
fluxes_bot22 <- calculate_regressions_custom_intervals(rawdata_fluxbot_22, start_end, "fluxbot", 768, 81)
fluxes_bot114 <- calculate_regressions_custom_intervals(rawdata_fluxbot_114, start_end, "fluxbot", 768, 81)
fluxes_bot108 <- calculate_regressions_custom_intervals(rawdata_fluxbot_108, start_end, "fluxbot", 768, 81)
fluxes_bot101 <- calculate_regressions_custom_intervals(rawdata_fluxbot_101, start_end, "fluxbot", 768, 81)
fluxes_bot112 <- calculate_regressions_custom_intervals(rawdata_fluxbot_112, start_end, "fluxbot", 768, 81)
fluxes_bot111 <- calculate_regressions_custom_intervals(rawdata_fluxbot_111, start_end, "fluxbot", 768, 81)
healthybots <- bind_rows(lst(
fluxes_bot100,
fluxes_bot13,
fluxes_bot22,
fluxes_bot114,
fluxes_bot108,
fluxes_bot101,
# fluxes_bot112,
fluxes_bot111),
.id = "id")
healthybots <- healthybots %>%
mutate(stand = "healthy")
# fluxbots in the diseased plot:
fluxes_bot113 <- calculate_regressions_custom_intervals(rawdata_fluxbot_113, start_end, "fluxbot", 768, 81)
fluxes_bot103 <- calculate_regressions_custom_intervals(rawdata_fluxbot_103, start_end, "fluxbot", 768, 81)
fluxes_bot102 <- calculate_regressions_custom_intervals(rawdata_fluxbot_102, start_end, "fluxbot", 768, 81)
fluxes_bot105 <- calculate_regressions_custom_intervals(rawdata_fluxboy_105, start_end, "fluxbot", 768, 81) #Error: object 'rawdata_fluxbot_105' not found; yeah there was a typo in the fluxbot name!
fluxes_bot24 <- calculate_regressions_custom_intervals(rawdata_fluxbot_24, start_end, "fluxbot", 768, 81)
fluxes_bot106 <- calculate_regressions_custom_intervals(rawdata_fluxbot_106, start_end, "fluxbot", 768, 81)
fluxes_bot104 <- calculate_regressions_custom_intervals(rawdata_fluxbot_104, start_end, "fluxbot", 768, 81)
fluxes_bot110 <- calculate_regressions_custom_intervals(rawdata_fluxbot_110, start_end, "fluxbot", 768, 81)
unhealthybots <- bind_rows(lst(fluxes_bot113, fluxes_bot103, fluxes_bot102, fluxes_bot105, fluxes_bot24, fluxes_bot106,
fluxes_bot104, fluxes_bot110), .id = "id")
unhealthybots <- unhealthybots %>%
mutate(stand = "unhealthy")
HF_fluxestimates <- rbind(healthybots, unhealthybots)
HF_fluxestimates <- HF_fluxestimates %>%
filter(hour_of_obs > "2023-10-02 00:00:00" &
hour_of_obs < "2023-11-17 00:00:00")
# select the flux estimate that was produced with the higher of the two beta values
HF_fluxestimates <- HF_fluxestimates %>%
mutate(final_flux_umolm2sec =
case_when(linear_beta > quadratic_beta ~ fluxL_umolm2sec,
quadratic_beta > linear_beta ~ fluxQ_umolm2sec))
write.csv(HF_fluxestimates, "HarvardForest_fluxestimates_fall2023.csv")
